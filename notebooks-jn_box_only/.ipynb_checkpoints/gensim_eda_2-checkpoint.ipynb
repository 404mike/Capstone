{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, codecs\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IterFile(object):\n",
    "    '''\n",
    "    class object to do the iterating on individual book txt documents, including file i/o.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        \n",
    "    def _open_file(self):\n",
    "        self.file = codecs.open(self.filepath, 'r', encoding='utf_8')\n",
    "        \n",
    "    def _close_file(self):\n",
    "        self.file.close()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        overwrite iteration to include file i/o\n",
    "        '''\n",
    "        self._open_file()\n",
    "        \n",
    "        for line in self.file:\n",
    "            yield line\n",
    "        \n",
    "        self._close_file()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_dir = '/Users/rachelbrynsvold/dsi/capstone_dir/top_100_dev_corp/books/clean/'\n",
    "outputs_dir = '/Users/rachelbrynsvold/dsi/capstone_dir/top_100_dev_corp/outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-clean.txt',\n",
       " '100-clean.txt',\n",
       " '105-clean.txt',\n",
       " '108-clean.txt',\n",
       " '1080-clean.txt',\n",
       " '11-clean.txt',\n",
       " '1112-clean.txt',\n",
       " '1184-clean.txt',\n",
       " '12-clean.txt',\n",
       " '120-clean.txt',\n",
       " '1232-clean.txt',\n",
       " '1260-clean.txt',\n",
       " '1322-clean.txt',\n",
       " '1342-clean.txt',\n",
       " '135-clean.txt',\n",
       " '1399-clean.txt',\n",
       " '140-clean.txt',\n",
       " '1400-clean.txt',\n",
       " '1404-clean.txt',\n",
       " '14264-clean.txt',\n",
       " '147-clean.txt',\n",
       " '1497-clean.txt',\n",
       " '15399-clean.txt',\n",
       " '158-clean.txt',\n",
       " '16-clean.txt',\n",
       " '160-clean.txt',\n",
       " '161-clean.txt',\n",
       " '16382-clean.txt',\n",
       " '1661-clean.txt',\n",
       " '1727-clean.txt',\n",
       " '174-clean.txt',\n",
       " '1952-yellow_wallpaper-clean.txt',\n",
       " '19942-clean.txt',\n",
       " '20-clean.txt',\n",
       " '20203-clean.txt',\n",
       " '203-clean.txt',\n",
       " '205-clean.txt',\n",
       " '21279-clean.txt',\n",
       " '2148-clean.txt',\n",
       " '2174-clean.txt',\n",
       " '219-clean.txt',\n",
       " '224-clean.txt',\n",
       " '23-clean.txt',\n",
       " '236-clean.txt',\n",
       " '2500-clean.txt',\n",
       " '25305-clean.txt',\n",
       " '2591-clean.txt',\n",
       " '2600-clean.txt',\n",
       " '2680-clean.txt',\n",
       " '2701-moby-clean.txt',\n",
       " '28054-clean.txt',\n",
       " '2814-clean.txt',\n",
       " '2852-clean.txt',\n",
       " '28520-clean.txt',\n",
       " '30360-clean.txt',\n",
       " '30601-clean.txt',\n",
       " '3207-clean.txt',\n",
       " '33-clean.txt',\n",
       " '33283-clean.txt',\n",
       " '345-clean.txt',\n",
       " '34901-clean.txt',\n",
       " '35-clean.txt',\n",
       " '36-clean.txt',\n",
       " '3600-clean.txt',\n",
       " '38427-clean.txt',\n",
       " '408-clean.txt',\n",
       " '41-clean.txt',\n",
       " '42-clean.txt',\n",
       " '4300-clean.txt',\n",
       " '4363-clean.txt',\n",
       " '45-clean.txt',\n",
       " '4517-clean.txt',\n",
       " '46-clean.txt',\n",
       " '514-clean.txt',\n",
       " '5200-clean.txt',\n",
       " '521-clean.txt',\n",
       " '526-clean.txt',\n",
       " '55-clean.txt',\n",
       " '55387-clean.txt',\n",
       " '55404-clean.txt',\n",
       " '6130-clean.txt',\n",
       " '730-clean.txt',\n",
       " '7370-clean.txt',\n",
       " '74-clean.txt',\n",
       " '76-clean.txt',\n",
       " '768-clean.txt',\n",
       " '829-clean.txt',\n",
       " '84-clean.txt',\n",
       " '844-clean.txt',\n",
       " '851-clean.txt',\n",
       " '863-clean.txt',\n",
       " '8800-clean.txt',\n",
       " '932-clean.txt',\n",
       " '98-clean.txt',\n",
       " '996-clean.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_corp = PlaintextCorpusReader(source_dir, '.*\\.txt')\n",
    "fileid_lst = temp_corp.fileids()\n",
    "fileid_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eda(transform_txt_file, fileid_lst=fileid_lst):\n",
    "    '''\n",
    "    Do transformations with updated transformation function and return all the eda items\n",
    "    '''\n",
    "    \n",
    "    all_transf_books_lst = [transform_txt_file(f) for f in fileid_lst]\n",
    "    \n",
    "    \n",
    "    book_lengths = [(tup[0], len(tup[1])) for tup in zip(fileid_lst, all_transf_books_lst)]\n",
    "    \n",
    "    avg_num_tokens = int(np.mean([len(book) for book in all_transf_books_lst]))\n",
    "    \n",
    "    \n",
    "    dictionary = corpora.Dictionary(all_transf_books_lst)\n",
    "    \n",
    "    dictionary_length = len(dictionary)\n",
    "    \n",
    "    \n",
    "    corpus = [dictionary.doc2bow(book) for book in all_transf_books_lst]\n",
    "    \n",
    "    \n",
    "    unique_toks_num_lst = [len(book) for book in corpus]\n",
    "\n",
    "    unique_toks_per_fileid = zip(fileid_lst, unique_toks_num_lst)\n",
    "    \n",
    "    avg_unique_toks = int(np.mean(unique_toks_num_lst))\n",
    "    \n",
    "    \n",
    "    return book_lengths, avg_num_tokens, dictionary, dictionary_length, unique_toks_per_fileid, avg_unique_toks\n",
    "\n",
    "\n",
    "def save_stuff(dictionary, corpus, distinguishing_str, outputs_dir='/Users/rachelbrynsvold/dsi/capstone_dir/top_100_dev_corp/outputs/'):\n",
    "    '''\n",
    "    Save the outputs of the most recent eda step\n",
    "    '''\n",
    "    \n",
    "    dictionary.save(outputs_dir + distinguishing_str + '.dict')\n",
    "    \n",
    "    corpora.MmCorpus.serialize(outputs_dir + distinguishing_str + '_corpus.mm', corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_txt_file_v1(fname, root=source_dir):\n",
    "    '''\n",
    "    Top-level function to call all of the subfunctions for text transformation\n",
    "    Assumes you want to remove empty lines and tokenize (because you do)\n",
    "    v1 just repeats v0 functionality with new subfunction architecture; checking to make sure I did it right\n",
    "    '''\n",
    "    fp = root + fname\n",
    "\n",
    "    book_as_lst = []\n",
    "    for line in IterFile(fp):\n",
    "        if empty_line_check(line) == True:\n",
    "            book_as_lst.extend(basic_tokenize(line))\n",
    "        \n",
    "    return book_as_lst\n",
    "     \n",
    "\n",
    "def empty_line_check(line) :\n",
    "    '''\n",
    "    checks for empty line\n",
    "    '''\n",
    "    if line == \"\\n\":\n",
    "        empty = True\n",
    "    else:\n",
    "        empty = False\n",
    "    return empty\n",
    "\n",
    "    \n",
    "def basic_tokenize(line):\n",
    "    '''\n",
    "    convert to list\n",
    "    strip punctuation, lowercase\n",
    "    '''\n",
    "    return [tok.lower().strip(punctuation) for tok in line.strip('\\n').split()]\n",
    "        \n",
    "            \n",
    "def remove_stop_words():\n",
    "    pass\n",
    "    \n",
    "def lemmatize():\n",
    "    pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_lengths, avg_num_tokens, dictionary, dictionary_length, unique_toks_per_fileid, avg_unique_toks = \\\n",
    "    eda(transform_txt_file_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
