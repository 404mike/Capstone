{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is an interface to conveniently create the run paramters file for either script (Dimensional Reduction or Modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/r0b00sj/Documents/Capstone/Capstone/py_scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../py_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from utils_streamers import DirFileMgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensional Reduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter unique ID string HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr_id_str = 'dr_500k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which data set will be used? Enter either '5000' or 'full': gpu-full\n",
      "corpus lst fp is assigned as  ../../outputs-git_ignored/dr_500k/dr_500k_lst.txt\n",
      "dictionary fp is assigned as  ../../outputs-git_ignored/dr_500k/dr_500k.dict\n",
      "counts dictionary fp is assigned as  ../../outputs-git_ignored/dr_500k/dr_500k_json.txt\n",
      "dimensional reduction run parameters fp is assigned as  ../../outputs-git_ignored/dr_500k/dr_500k_dr_run_params.txt\n"
     ]
    }
   ],
   "source": [
    "dr_fps = DirFileMgr(dr_id_str)\n",
    "dr_fps.create_all_dr_fps(new_setup='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set run parameters for the dimensional reduction stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Minimum number of documents that a token must appear in (otherwise it is filtered out)\n",
    "min_freq = 5  #Default 5; set to None to skip frequency filtering\n",
    "\n",
    "#Maximum percentage of the corpus in which a token can appear (otherwise it is filtered out)\n",
    "max_freq = 0.5  #Default 0.5\n",
    "\n",
    "#Number of words to keep in dictionary\n",
    "keep_n = 500000  #Gensim default = 100000; I changed it here since our docs are so large (books)\n",
    "\n",
    "#Dataset used\n",
    "dataset = dr_fps.source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr_d = { 'min_freq' : min_freq, 'max_freq' : max_freq, 'keep_n' : keep_n, \\\n",
    "       'dataset' : dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(dr_d, open(dr_fps.dr_run_params, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter unique ID string HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_id_str = 'gpu_mod_run_8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus lst fp is assigned as  ../../outputs-git_ignored/gpu_mod_run_8/gpu_mod_run_8_lst.txt\n",
      "dictionary fp is assigned as  ../../outputs-git_ignored/gpu_mod_run_8/gpu_mod_run_8.dict\n",
      "counts dictionary fp is assigned as  ../../outputs-git_ignored/gpu_mod_run_8/gpu_mod_run_8_json.txt\n",
      "model fp is assigned as  ../../outputs-git_ignored/gpu_mod_run_8/gpu_mod_run_8.model\n",
      "modeling run parameters fp is assigned as  ../../outputs-git_ignored/gpu_mod_run_8/gpu_mod_run_8_mod_run_params.txt\n"
     ]
    }
   ],
   "source": [
    "mod_fps = DirFileMgr(mod_id_str)\n",
    "mod_fps.create_all_modeling_fps(mod_id_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set run parameters for the modeling stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, gensim defaults:\n",
    "corpus=None, num_topics=100, id2word=None, workers=None, chunksize=2000, passes=1, batch=False, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, random_state=None, minimum_probability=0.01, minimum_phi_value=0.01, per_word_topics=False, dtype=type 'numpy.float32'\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamulticore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 * 8 * terms * topics\n",
    "\n",
    "#Number of topics for LDA to train on\n",
    "num_topics = 20  #Project default: 50 (for entire corpus)\n",
    "\n",
    "#Number of cores on the machine running the modeling script MINUS 1\n",
    "workers = 40 #Common values: \n",
    "            #recent macbook=2, \n",
    "            #recent macbook pro= 4 or 2\n",
    "            #recent macbook air=2\n",
    "            #t2.xlarge ec2=4\n",
    "            #Spec too few cores and run is computationally sub-optimized\n",
    "            #Spec too many and run may fail\n",
    "#Workers will be equal to number of cores - 1\n",
    "\n",
    "#Number of docs to process at a time\n",
    "chunksize = 2000  #Default 2000\n",
    "\n",
    "#Number of training passes for LDA algorithm\n",
    "passes = 1  #Default 1\n",
    "#10 might be high, esp for large corpus - I see 1-5 type values\n",
    "\n",
    "#If batch is not set, perform online training by updating the model once \n",
    "#every workers * chunksize documents (online training). \n",
    "#Otherwise, run batch LDA, updating model only once at the end of each full corpus pass.\n",
    "batch = False\n",
    "\n",
    "#alpha and eta are hyperparameters that affect sparsity of the document-topic (theta) \n",
    "#and topic-word (lambda) distributions. Both default to a symmetric \n",
    "#1.0/num_topics prior.\n",
    "\n",
    "#alpha can be set to an explicit array = prior of your choice. It also \n",
    "#support special values of ‘asymmetric’ and ‘auto’: the former uses a \n",
    "#fixed normalized asymmetric 1.0/topicno prior, the latter learns an \n",
    "#asymmetric prior directly from your data.\n",
    "alpha = 'asymmetric'\n",
    "\n",
    "#eta can be a scalar for a symmetric prior over topic/word distributions, \n",
    "#or a matrix of shape num_topics x num_words, which can be used to impose \n",
    "#asymmetric priors over the word distribution on a per-topic basis. This \n",
    "#may be useful if you want to seed certain topics with particular words by \n",
    "#boosting the priors for those words.\n",
    "eta = None\n",
    "\n",
    "#decay and offset parameters are the same as Kappa and Tau_0 in Hoffman \n",
    "#et al, respectively.\n",
    "#http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf\n",
    "decay = 0.5\n",
    "offset = 1.0\n",
    "\n",
    "#Calculate and log perplexity estimate from the latest mini-batch once \n",
    "#every eval_every documents. Set to None to disable perplexity estimation \n",
    "#(faster), or to 0 to only evaluate perplexity once, at the end of each \n",
    "#corpus pass.\n",
    "eval_every = 10\n",
    "\n",
    "iterations = 50\n",
    "#https://groups.google.com/forum/#!topic/gensim/aGXc0qiVBhU\n",
    "\n",
    "#iterations continue until the difference between two consecutive topic (gamma) \n",
    "#estimates is less than `gamma_threshold`\n",
    "gamma_threshold = 0.001\n",
    "#https://groups.google.com/forum/#!topic/gensim/aGXc0qiVBhU\n",
    "\n",
    "random_state = None\n",
    "\n",
    "#controls filtering the topics returned for a document\n",
    "minimum_probability = 0.01\n",
    "\n",
    "minimum_phi_value = 0.01\n",
    "\n",
    "per_word_topic = False\n",
    "\n",
    "#-----------\n",
    "#How often to do the maximization step as related to chunk size - for single LDA only\n",
    "update_every = 0  #Default 0\n",
    "#See this link for more detail: https://groups.google.com/forum/#!topic/gensim/ojySenxQHi4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 'asymmetric',\n",
       " 'batch': False,\n",
       " 'chunksize': 2000,\n",
       " 'decay': 0.5,\n",
       " 'eta': None,\n",
       " 'eval_every': 10,\n",
       " 'gamma_threshold': 0.001,\n",
       " 'iterations': 50,\n",
       " 'minimum_phi_value': 0.01,\n",
       " 'minimum_probability': 0.01,\n",
       " 'num_topics': 20,\n",
       " 'offset': 1.0,\n",
       " 'passes': 1,\n",
       " 'per_word_topic': 0.01,\n",
       " 'random_state': None,\n",
       " 'update_every': 0,\n",
       " 'workers': 41}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_d = {'num_topics' : num_topics, \\\n",
    "         'workers' : cores, \\\n",
    "         'chunksize' : chunksize, \\\n",
    "         'passes' : passes, \\\n",
    "         'batch' : batch,\\\n",
    "         'alpha' : alpha, \\\n",
    "         'eta' : eta, \\\n",
    "         'decay' : decay, \\\n",
    "         'offset' : offset, \\\n",
    "         'eval_every' : eval_every, \\\n",
    "         'iterations' : iterations, \\\n",
    "         'gamma_threshold' : gamma_threshold, \\\n",
    "         'random_state' : random_state, \\\n",
    "         'minimum_probability' : minimum_probability, \\\n",
    "         'minimum_phi_value' : minimum_phi_value, \\\n",
    "         'per_word_topic' : minimum_phi_value, \\\n",
    "         'update_every' : update_every }\n",
    "mod_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(mod_d, open(mod_fps.mod_run_params, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keep_n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b5e5e5b86f2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#rule of thumb for memory reqts is 8 bytes per term per topic:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;36m8\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkeep_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#if this number is higher than your available memory, need to limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#terms and/or topics, or get more memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keep_n' is not defined"
     ]
    }
   ],
   "source": [
    "#rule of thumb for memory reqts is 8 bytes per term per topic:\n",
    "8 * keep_n * num_topics\n",
    "#if this number is higher than your available memory, need to limit  \n",
    "#terms and/or topics, or get more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
