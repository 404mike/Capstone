{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is to load, inspect, and play with your model!  Enjoy!\n",
    "\n",
    "__Please don't hesitate to reach out with questions, bug fixes, new findings - whatever!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1:  Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rachelbrynsvold/dsi/capstone_dir/Capstone/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you are here ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rachelbrynsvold/dsi/capstone_dir/Capstone/py_scripts\n"
     ]
    }
   ],
   "source": [
    "cd ../py_scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py                   fit_gensim_lda.py\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m/                  fit_gensim_lda.py.bak\r\n",
      "dimensional_reduction.py      metadata_extraction.py\r\n",
      "dimensional_reduction.py.bak  metadata_extraction.pyc\r\n",
      "fit_doc2vec.py                utils_streamers.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-16 14:56:18,596 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "#before pres: finalize me - remove unused modules\n",
    "import os, codecs\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import ldamodel\n",
    "import itertools\n",
    "from itertools import islice\n",
    "import random\n",
    "import utils_streamers\n",
    "from utils_streamers import DirFileMgr, IterFile, CorpStreamer, BOWCorpStreamer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter string used for identifier for saving model:\n",
    "\n",
    "**This should be the one user-input item needed to run this notebook on a user-created model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr_mod_id_str = \"ff_50_15p\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up needed file managment objects, paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus lst fp is assigned as  ../outputs-git_ignored/ff_50_15p/ff_50_15p_lst.txt\n",
      "dictionary fp is assigned as  ../outputs-git_ignored/ff_50_15p/ff_50_15p.dict\n",
      "counts dictionary fp is assigned as  ../outputs-git_ignored/ff_50_15p/ff_50_15p_json.txt\n",
      "model fp is assigned as  ../outputs-git_ignored/ff_50_15p/ff_50_15p.model\n",
      "pyLDAvis html fp is assigned as  ../outputs-git_ignored/ff_50_15p/ff_50_15p.model\n"
     ]
    }
   ],
   "source": [
    "fps = DirFileMgr(dr_mod_id_str)\n",
    "fps.create_all_modeling_fps(dr_mod_id_str)\n",
    "fps.add_fp('pyLDAvis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../outputs-git_ignored/ff_50_15p'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps.working_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy + paste the above next to the ls to review the contents of the directory that should contain the dictionary and token-list corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff_50_15p.dict                   ff_50_15p_corpus.mm.index\r\n",
      "ff_50_15p.model                  ff_50_15p_json.txt\r\n",
      "ff_50_15p.model.expElogbeta.npy  ff_50_15p_ldavis.html\r\n",
      "ff_50_15p.model.id2word          ff_50_15p_lst.txt\r\n",
      "ff_50_15p.model.state            ff_50_15p_txt_corpus.mm\r\n",
      "ff_50_15p_corpus.mm              \u001b[34mtmp_dicts\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls ../outputs-git_ignored/ff_50_15p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../outputs-git_ignored/ff_50_15p'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fps.model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If different than your 'working dir', copy + paste the model_dir string next to the ls to review the contents of the directory that should contain your model and supporting files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff_50_15p.dict                   ff_50_15p_corpus.mm.index\r\n",
      "ff_50_15p.model                  ff_50_15p_json.txt\r\n",
      "ff_50_15p.model.expElogbeta.npy  ff_50_15p_ldavis.html\r\n",
      "ff_50_15p.model.id2word          ff_50_15p_lst.txt\r\n",
      "ff_50_15p.model.state            ff_50_15p_txt_corpus.mm\r\n",
      "ff_50_15p_corpus.mm              \u001b[34mtmp_dicts\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls ../outputs-git_ignored/ff_50_15p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Load and Create Objects\n",
    "\n",
    "_Here we'll get all the objects we need for the following sections of the notebook_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load dictionary and create streamed corpus (generator) objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-16 14:57:29,383 : INFO : loading Dictionary object from ../outputs-git_ignored/ff_50_15p/ff_50_15p.dict\n",
      "2017-11-16 14:57:29,524 : INFO : loaded ../outputs-git_ignored/ff_50_15p/ff_50_15p.dict\n"
     ]
    }
   ],
   "source": [
    "d = corpora.Dictionary.load(fps.dictionary_fp)\n",
    "c = CorpStreamer(d, fps.corp_lst_fp, inc_title='Y')\n",
    "bow_c = BOWCorpStreamer(d, fps.corp_lst_fp, inc_title='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is a little on-off switch for my various long run-time operations that are behind a check.\n",
    "go_no_go = 0  #GO\n",
    "#go_no_go = 1 #NO_GO - ie do NOT run all the long run-time items.  Note that some of these long\n",
    "                #runtime items will be necessary once each time after the kernel is reset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create serialized corpus, so we can quickly index in and grab books we want**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-16 14:57:55,847 : INFO : storing corpus in Matrix Market format to ../outputs-git_ignored/ff_50_15p/ff_50_15p_corpus.mm\n",
      "2017-11-16 14:57:55,858 : INFO : saving sparse matrix to ../outputs-git_ignored/ff_50_15p/ff_50_15p_corpus.mm\n",
      "2017-11-16 14:57:55,873 : INFO : PROGRESS: saving document #0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus fp is assigned as  ../outputs-git_ignored/ff_50_15p/ff_50_15p_corpus.mm\n",
      "../outputs-git_ignored/ff_50_15p/ff_50_15p_corpus.mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-16 14:58:17,845 : INFO : PROGRESS: saving document #1000\n",
      "2017-11-16 14:58:38,966 : INFO : PROGRESS: saving document #2000\n",
      "2017-11-16 14:58:59,778 : INFO : PROGRESS: saving document #3000\n",
      "2017-11-16 14:59:22,449 : INFO : PROGRESS: saving document #4000\n",
      "2017-11-16 14:59:39,872 : INFO : PROGRESS: saving document #5000\n",
      "2017-11-16 15:00:00,134 : INFO : PROGRESS: saving document #6000\n",
      "2017-11-16 15:00:18,563 : INFO : PROGRESS: saving document #7000\n",
      "2017-11-16 15:00:36,950 : INFO : PROGRESS: saving document #8000\n",
      "2017-11-16 15:00:57,349 : INFO : PROGRESS: saving document #9000\n",
      "2017-11-16 15:01:19,491 : INFO : PROGRESS: saving document #10000\n",
      "2017-11-16 15:01:37,088 : INFO : PROGRESS: saving document #11000\n",
      "2017-11-16 15:01:51,629 : INFO : PROGRESS: saving document #12000\n",
      "2017-11-16 15:02:08,355 : INFO : PROGRESS: saving document #13000\n",
      "2017-11-16 15:02:26,379 : INFO : PROGRESS: saving document #14000\n",
      "2017-11-16 15:02:48,719 : INFO : PROGRESS: saving document #15000\n",
      "2017-11-16 15:03:05,954 : INFO : PROGRESS: saving document #16000\n",
      "2017-11-16 15:03:23,185 : INFO : PROGRESS: saving document #17000\n",
      "2017-11-16 15:03:40,440 : INFO : PROGRESS: saving document #18000\n",
      "2017-11-16 15:03:56,200 : INFO : PROGRESS: saving document #19000\n",
      "2017-11-16 15:04:13,342 : INFO : PROGRESS: saving document #20000\n",
      "2017-11-16 15:04:31,388 : INFO : PROGRESS: saving document #21000\n",
      "2017-11-16 15:04:51,218 : INFO : PROGRESS: saving document #22000\n",
      "2017-11-16 15:05:15,822 : INFO : PROGRESS: saving document #23000\n",
      "2017-11-16 15:05:33,558 : INFO : PROGRESS: saving document #24000\n",
      "2017-11-16 15:05:55,030 : INFO : PROGRESS: saving document #25000\n",
      "2017-11-16 15:06:16,754 : INFO : PROGRESS: saving document #26000\n",
      "2017-11-16 15:06:39,113 : INFO : PROGRESS: saving document #27000\n",
      "2017-11-16 15:07:02,238 : INFO : PROGRESS: saving document #28000\n",
      "2017-11-16 15:07:14,776 : INFO : saved 28492x100000 matrix, density=2.747% (78270912/2849200000)\n",
      "2017-11-16 15:07:14,784 : INFO : saving MmCorpus index to ../outputs-git_ignored/ff_50_15p/ff_50_15p_corpus.mm.index\n",
      "2017-11-16 15:07:14,807 : INFO : loaded corpus index from ../outputs-git_ignored/ff_50_15p/ff_50_15p_corpus.mm.index\n",
      "2017-11-16 15:07:14,810 : INFO : initializing corpus reader from ../outputs-git_ignored/ff_50_15p/ff_50_15p_corpus.mm\n",
      "2017-11-16 15:07:14,818 : INFO : accepted corpus with 28492 documents, 100000 features, 78270912 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "fps.add_fp('gensim_corpus')\n",
    "print (fps.gensim_corp_fp)\n",
    "\n",
    "#WARNING: this cell could take a while (several minutes - an hr+) if you\n",
    "# allow the if statement to be true and run the serialization.\n",
    "if 0 == go_no_go:\n",
    "    corpora.MmCorpus.serialize(fps.gensim_corp_fp, bow_c)\n",
    "    \n",
    "#whether the serialized corpus has just been already been created and saved (above), \n",
    "# or was created and saved earlier (as for demo) we load it with this expression:\n",
    "serial_corp = corpora.MmCorpus(fps.gensim_corp_fp)\n",
    "\n",
    "#Assign the total number of documents, while we are here:\n",
    "num_docs = len(serial_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-16 15:12:54,374 : INFO : loading LdaModel object from ../outputs-git_ignored/ff_50_15p/ff_50_15p.model\n",
      "2017-11-16 15:12:54,396 : INFO : loading expElogbeta from ../outputs-git_ignored/ff_50_15p/ff_50_15p.model.expElogbeta.npy with mmap=None\n",
      "2017-11-16 15:12:54,469 : INFO : setting ignored attribute id2word to None\n",
      "2017-11-16 15:12:54,473 : INFO : setting ignored attribute state to None\n",
      "2017-11-16 15:12:54,476 : INFO : setting ignored attribute dispatcher to None\n",
      "2017-11-16 15:12:54,479 : INFO : loaded ../outputs-git_ignored/ff_50_15p/ff_50_15p.model\n",
      "2017-11-16 15:12:54,482 : INFO : loading LdaModel object from ../outputs-git_ignored/ff_50_15p/ff_50_15p.model.state\n",
      "2017-11-16 15:12:54,644 : INFO : loaded ../outputs-git_ignored/ff_50_15p/ff_50_15p.model.state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=100000, num_topics=50, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "lda = ldamodel.LdaModel.load(fps.model_fp)\n",
    "print(lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Create a list of (index, book title) tuples, so we can more easily go back and forth btwn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This cell can take several mins to run\n",
    "if 0 == go_no_go:\n",
    "    title_lst = [(num, book[0]) for num, book in enumerate(c)]\n",
    "len(title_lst), title_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is a really simple little class object that I'll use to keep track of book titles and indexes for later\n",
    "class InterestingBooks(object):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bks = InterestingBooks()\n",
    "\n",
    "bks.pandp = (title_lst[3063])\n",
    "bks.sands = (title_lst[5418])\n",
    "bks.sherlock = (title_lst[5885])\n",
    "bks.moby = (title_lst[4431])\n",
    "bks.scientific_am = (title_lst[4475])\n",
    "bks.kjv_bible = (title_lst[1])\n",
    "bks.decl_ind = (title_lst[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Object Exploration\n",
    "\n",
    "_This notebook contains a variety of python, gensim, and custom objects.  I personally find it incredibly helpful (if not essential) to understand what is *in* all the objects I\"m working with.  If you're not too concerned with this, feel free to skip this section entirely._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Peek into the streamed tokenized corpus object**    \n",
    "\n",
    "By calling next(iter()) on the corpus, we can see the first document in it - it should be the Declaration of Independence.  The format of the document is a list of tokens, with the document title at index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(next(iter(c)))) \n",
    "next(iter(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore the Dictionary (aka Vocabulary)**  \n",
    "\n",
    "The dictionary is where each word assigned a numerical designation, which will be used to create vector representations of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, d[41], d[189], len(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Peek into the streamed bag-of-words corpus object**  \n",
    "\n",
    "Similarly to the streamed tokenized corpus, we can see the first document in the streamed bag-of-words corpus (Declaration of Independence again).  A bag-of-words document is a numerical (vector) representation of a text document, created by replacing each word with the number assigned to it in the dictionary.  If a word is repeated, the count is incremented.   \n",
    "\n",
    "_So in The Declaration of Independence, the word 'immunities' (word #41) occurs only once, while the word 'concurrence' (word #189) occurs three times._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(next(iter(bow_c))))\n",
    "next(iter(bow_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index into the serialized corpus**   \n",
    "Can directly index to this format of corpus, rather than iterate to prompt a generator to yield a document (as in the other two versions). \n",
    "\n",
    "_This is why we made it!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serial_corp[0], serial_corp[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Model Exploration, Visualization   \n",
    "\n",
    "_This is where it gets fun!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See all of the topics identified by the model, and their most significant terms**   \n",
    "\n",
    "Note that the LDA algorithm does not assign names/designations to the topics - it just identifies that certain words occur together and puts them into a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_topics_words = lda.print_topics(num_topics=lda.num_topics)\n",
    "all_topics_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired, you can pick some (or all) of the topics above and assign them designations, being sure the number and designation match positions in their list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is my assignment for a specific LDA model \n",
    "#will not be applicable to a new lda run - different documents, different topic number, etc\n",
    "# topic_nums = [1, 2, 4, 5, 8, 9, 10, 15, 18, 22, 24, 29, 35, 36, 37, 3, 39, 42, 47, 48, 49]\n",
    "# topic_designations = ['science', 'japan', 'french language', 'textiles', 'roman', \\\n",
    "#                       'french royalty', 'plants', 'christmas', 'chemistry', 'english gentry', \\\n",
    "#                      'ship', 'production', 'baking', 'french', 'ireland', 'middle east', \\\n",
    "#                      'american government', 'classical music', 'european war', 'early american war', \\\n",
    "#                       'poetry', 'catholic church', 'judeo-christian', 'spanish colonialism']\n",
    "\n",
    "topic_dict = {1 : 'science', 2 : 'japan', 4 : 'enineering', 5 : 'french language', \\\n",
    "              8 : 'textiles', 9 : 'roman', 10 : 'french royalty', 13 : 'eastern religion', \\\n",
    "              14 : 'german names', 15 : 'plants', 16 : 'christmas', 17 : 'nature - forest', \\\n",
    "              18 : 'chemistry', 19 : 'old english language', 20 : 'slavery', 22 : 'english gentry', \\\n",
    "              24 : 'ship', 25 : 'production', 26 : 'east asia', 28 : 'spanish language', \\\n",
    "              29 : 'cooking/baking', 32 : 'economics/sociology', 33 : 'missionaries', \\\n",
    "              34 : 'french', 35 : 'irish', 36 : 'middle east', 37 : 'american government', \\\n",
    "              38 : 'classical music', 39 : 'european war', 40 : 'christianity', 41 : 'gypsy', \\\n",
    "              42 : 'early american war', 43 : 'romantic poets', 44 : 'poetry', \\\n",
    "              45 : 'american wild west', 47 : 'european church-and-state', \\\n",
    "              48 : 'judeo-christian', 49 : 'spanish colonialism'}\n",
    "    \n",
    "#     pr[0] : pr[1] for pr in zip(topic_nums, topic_designations)}\n",
    "\n",
    "len(topic_dict), topic_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the LDA model with pyLDAvis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WARNING: This cell will take a while to execute - up to a couple of hours\n",
    "%time\n",
    "if 0==1:\n",
    "    lda_vis_serialized = pyLDAvis.gensim.prepare(lda, serial_corp, d)\n",
    "    pyLDAvis.save_html(lda_vis_serialized, fps.pyldavis_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Double-click on the file that is saved at the filepath shown below - \n",
    "#This will launch your interactive pyldavis chart in a browser window!!\n",
    "fps.pyldavis_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Alternatively, you can try to load it in the notebook, but I have always gotten a warning when I tried to do this with my LDA models, and I haven't wanted to deal with changing config variables and possibly crashing the notebook.\n",
    "\n",
    "\n",
    "But here's the command to display inline, if you want to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 0 == go_no_go:   \n",
    "    pyLDAvis.display(lda_vis_serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the model's word/document/topic relationships\n",
    "\n",
    "Check out the varieties of relationships between words, documents, and topics that the LDA algorithm has identified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper function to display the manually assigned topic designations for a document's topic vector\n",
    "def get_topic_desig(doc_topics_lst, topic_dict=topic_dict):\n",
    "    '''\n",
    "    Translate the numerical topics in a '.get_document_topics' list into the assigned topic designation \n",
    "    '''\n",
    "    descriptive_topic_lst = []\n",
    "    \n",
    "    for pr in doc_topics_lst:\n",
    "        if pr[0] in topic_dict: \n",
    "            descriptive_topic_lst.append((topic_dict[pr[0]], pr[1]))\n",
    "        else: \n",
    "            descriptive_topic_lst.append(('unassigned_topic', pr[1]))\n",
    "    \n",
    "    return descriptive_topic_lst\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most relevant _topics_ for a _document_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore this using one of our pre-selected books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = bks.kjv_bible\n",
    "#doc = bks.decl_ind\n",
    "#doc = bks.moby\n",
    "\n",
    "doc_ind = doc[0]\n",
    "print(doc_ind)\n",
    "doc_words = next(islice(c, doc_ind, doc_ind+1))\n",
    "doc_vec  = serial_corp[doc_ind]\n",
    "\n",
    "print(\"Title:\", doc_words[0])\n",
    "print(doc_words [1:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_topics = lda.get_document_topics(doc_vec[1:], minimum_probability=None)\n",
    "doc_topics, get_topic_desig(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in doc_topics:\n",
    "    print (lda.show_topic(n[0]))\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And just for fun, in case you want to try that same code on a random example, here is some code to pull a random document out of our corpus by taking a slice at a random number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_ind = random.randint(0, num_docs)\n",
    "print(rand_ind)\n",
    "\n",
    "#WARNING: potentially long run time (5-10mins)\n",
    "if 0 == go_no_go:\n",
    "    rand_doc_words = next(islice(c, rand_ind, rand_ind+1))\n",
    "    rand_doc_vec  = serial_corp[rand_ind]\n",
    "\n",
    "print(\"Title:\", rand_doc_words[0])\n",
    "print(rand_doc_words [1:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_doc_topics = lda.get_document_topics(rand_doc_vec[1:], \\\n",
    "                        minimum_probability=None)\n",
    "rand_doc_topics, get_topic_desig(rand_doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in rand_doc_topics:\n",
    "    print (lda.show_topic(n[0]))\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most relevent/likely _topics_ for a _word_ (aka 'term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "france_tops = lda.get_term_topics(d.token2id['france'], minimum_probability=0.001)\n",
    "france_tops, get_topic_desig(france_tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in [tup[0] for tup in france_tops]:\n",
    "    print (lda.print_topic(n))\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_word = random.randint(0, len(d))\n",
    "rand_word, d[rand_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tweak the 'minimum probability' argument to expand/contract the list\n",
    "term_tops = lda.get_term_topics(rand_word, minimum_probability=.00001)\n",
    "term_tops, get_topic_desig(term_tops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [tup[0] for tup in term_tops]:\n",
    "    print (lda.print_topic(n))\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will compare books in the corpus.  We will accomplish this by using cosine similarity to compare the topic vector representations of these documents!\n",
    "\n",
    "We can also add and subtract books, and find the books in the corpus that are most similar to the resulting sum or difference!\n",
    "\n",
    "The 'magic' that makes this all work is that our lda model has allowed us to represent each book as a mixture of n different topics, where n is the number of topics that the model is trained on.  If our topic model is good, then books that have similar probabilities of the same topics should be similar to each other!  \n",
    "\n",
    "_In effect we have reduced the number of dimensions required to represent an *entire book* to around 50 or less!!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some helper functions for vector addition, subtraction, and comparision\n",
    "def vec_lst_add_subtract(top_vec_1, top_vec_2, mode='add'):\n",
    "    '''\n",
    "    Utility to add and subtract vectors in the format of a list of tuples\n",
    "    '''\n",
    "    res_vec = []\n",
    "    \n",
    "    if mode == 'add':\n",
    "        for pr1, pr2 in zip(top_vec_1, top_vec_2):\n",
    "            sm = pr1[1] + pr2[1]\n",
    "            if sm > 1:\n",
    "                sm = 1\n",
    "            res_vec.append((pr1[0], sm))\n",
    "    elif mode == 'subtract':\n",
    "        for pr1, pr2 in zip(top_vec_1, top_vec_2):\n",
    "            diff = pr1[1] - pr2[1]\n",
    "            if diff < 0:\n",
    "                diff = .0000000000000000001\n",
    "            res_vec.append((pr1[0], diff))\n",
    "    else:\n",
    "        print(\"Invalid mode\")\n",
    "        return 0\n",
    "    \n",
    "    return res_vec\n",
    "\n",
    "\n",
    "def find_most_similar(sim_vec, all_top_vecs, title_lst, vec_in_corp='Y', n_results=10):                \n",
    "    '''\n",
    "    Calculates cosine similarity across the entire corpus and returns \n",
    "    the n_results number of most similar documents\n",
    "    '''\n",
    "    \n",
    "    cos_sims = [gensim.matutils.cossim(sim_vec, vec) for vec in all_top_vecs]\n",
    "    \n",
    "    if vec_in_corp == 'N':\n",
    "        most_similar_ind = np.argsort(cos_sims)[::-1][:n_results]\n",
    "    if vec_in_corp == 'Y':\n",
    "        most_similar_ind = np.argsort(cos_sims)[::-1][:n_results+1][1:]\n",
    "        #exclude 'self', in the case that it is a book in the corpus\n",
    "    \n",
    "    for ind in most_similar_ind:\n",
    "        print (title_lst[ind], cos_sims[ind])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab all the document topic vectors and stick them in a list that we can use later.\n",
    "\n",
    "_Turns out it's actually relatively computationally intensive to 'get_document_topics' from the lda model - my understanding is that the model doesn't store these vectors and so each time we ask it for a document topic vector, it has to go and calculate the probability for each topic based on the bow representation of the entire book.  No wonder it takes a while!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WARNING: Long execution time - 20-30 mins\n",
    "if 0 == go_no_go:\n",
    "    all_top_vecs = [lda.get_document_topics(serial_corp[n], minimum_probability=0) \\\n",
    "                    for n in range(len(serial_corp))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we represent the book or books that we're examining as vector that shows the probability  of it containing each of the topics of our lda topic model.  See below for an example of one of these vectors.\n",
    "\n",
    "_Note we ensure that the vector has a length of 50 (ie contains probabilities for all 50 topics) by setting 'minimum_probability=0'.  This is important because we want to be comparing same-length vectors for this implementation of addition/subtraction/similarity._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Book Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandp_top_vec = lda.get_document_topics(serial_corp[bks.pandp[0]], minimum_probability=0)\n",
    "print(bks.pandp)\n",
    "pandp_top_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_most_similar(pandp_top_vec, all_top_vecs, title_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Whoa, we identified two other Jane Austen books as similar to Pride & Prejudice!!  That is cool!!**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's check out what our model thinks is similar to the king james bible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bible_top_vec = lda.get_document_topics(serial_corp[bks.kjv_bible[0]], minimum_probability=0)\n",
    "\n",
    "find_most_similar(bible_top_vec, all_top_vecs, title_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Book Algebra!**\n",
    "\n",
    "_**The Grand Finale**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff_vec = vec_lst_add_subtract(pandp_vec, bible_vec, mode='subtract')\n",
    "diff_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_most_similar(diff_vec, all_top_vecs, title_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_vec = vec_lst_add_subtract(pandp_vec, bible_vec, mode='add')\n",
    "find_most_similar(sum_vec, all_top_vecs, title_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
