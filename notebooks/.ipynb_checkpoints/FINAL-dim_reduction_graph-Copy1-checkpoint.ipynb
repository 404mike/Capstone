{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, codecs\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#you are here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/vagrant/Capstone/notebooks-local_only'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the 'header' assignment doesn't match this ^, you will probably need to copy + paste this string and assign it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This should be all the information needed for this notebook's relative path assignment!\n",
    "Confirm the dist_string matches the one you entered for your dimensional reduction and model fitting   \n",
    "Confirm the header for the outputs_dir   \n",
    "Make sure the outputs_dir exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vagrant/Capstone/outputs-git_ignored/ff_50_15p/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = '/home/vagrant/Capstone/outputs-git_ignored' + '/'\n",
    "\n",
    "dist_string = 'ff_50_15p'\n",
    "\n",
    "outputs_dir = header + dist_string + '/'\n",
    "outputs_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again copy + paste the above next to the cd, to change to the directory of interest and review the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/Capstone/outputs-git_ignored/ff_50_15p\n"
     ]
    }
   ],
   "source": [
    "cd /home/vagrant/Capstone/outputs-git_ignored/ff_50_15p/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check what's in your outputs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff_50_15p.dict                   ff_50_15p.model.id2word  ff_50_15p_lst.txt\r\n",
      "ff_50_15p.model                  ff_50_15p.model.state    \u001b[0m\u001b[01;34mtmp_dicts\u001b[0m/\r\n",
      "ff_50_15p.model.expElogbeta.npy  ff_50_15p_json.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j_fp = outputs_dir + dist_string + '_json.txt'\n",
    "c_fp = outputs_dir + dist_string + 'dim_r_chart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"tokenized\": {\"avg_unique\": 4432, \"total_vocab\": 1278171, \"avg_words\": 48190}, \"tok_and_sw\": {\"avg_unique\": 4318, \"total_vocab\": 1278018, \"avg_words\": 20614}, \"freq_filtered\": {\"avg_unique\": 2747, \"total_vocab\": 100000, \"avg_words\": 6775}}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(j_fp) as f:\n",
    "    d = f.read()\n",
    "    \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"tokenized\": {\"avg_unique\": 4432, \"total_vocab\": 1278171, \"avg_words\": 48190}, \"tok_and_sw\": {\"avg_unique\": 4318, \"total_vocab\": 1278018, \"avg_words\": 20614}, \"freq_filtered\": {\"avg_unique\": 2747, \"total_vocab\": 100000, \"avg_words\": 6775}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average unique words per book: \n",
      "    Initial (tokenized):  4432\n",
      "    Stop words removed:  4318\n",
      "    Frequency filtered:  2747\n",
      "   \n",
      "Average word count per book: \n",
      "    Initial (tokenized):  48190\n",
      "    Stop words removed:  20614\n",
      "    Frequency filtered:  6775\n",
      "   \n",
      "Vocabulary length: \n",
      "    Initial (tokenized):  1278171\n",
      "    Stop words removed:  1278018\n",
      "    Frequency filtered:  100000\n"
     ]
    }
   ],
   "source": [
    "print \"Average unique words per book: \"\n",
    "print \"   \", \"Initial (tokenized): \", d['tokenized']['avg_unique']\n",
    "print \"   \", \"Stop words removed: \", d['tok_and_sw']['avg_unique']\n",
    "print \"   \", \"Frequency filtered: \", d['freq_filtered']['avg_unique']\n",
    "print \"   \"\n",
    "\n",
    "print \"Average word count per book: \"\n",
    "print \"   \", \"Initial (tokenized): \", d['tokenized']['avg_words']\n",
    "print \"   \", \"Stop words removed: \", d['tok_and_sw']['avg_words']\n",
    "print \"   \", \"Frequency filtered: \", d['freq_filtered']['avg_words']\n",
    "print \"   \"\n",
    "\n",
    "print \"Vocabulary length: \"\n",
    "print \"   \", \"Initial (tokenized): \", d['tokenized']['total_vocab']\n",
    "print \"   \", \"Stop words removed: \", d['tok_and_sw']['total_vocab']\n",
    "print \"   \", \"Frequency filtered: \", d['freq_filtered']['total_vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1278171, 48190, 4432], [1278018, 20614, 4318], [100000, 6775, 2747])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_keys_lst = ['total_vocab', 'avg_words', 'avg_unique']\n",
    "tokenized_lst = [d['tokenized'][key] for key in sec_keys_lst]\n",
    "stop_words_removed_lst = [d['tok_and_sw'][key] for key in sec_keys_lst]\n",
    "frequency_filtered_lst = _lst = [d['freq_filtered'][key] for key in sec_keys_lst]\n",
    "tokenized_lst, stop_words_removed_lst, frequency_filtered_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_vocab', 'avg_words', 'avg_unique']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_keys_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dim_reduction_chart(d, tokenized_lst=tokenized_lst, \\\n",
    "                            stop_words_removed_lst=stop_words_removed_lst, \\\n",
    "                            frequency_filtered_lst=frequency_filtered_lst, \\\n",
    "                           c_fp=None):   \n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    n_groups = 3\n",
    "    pos = list(range(n_groups))\n",
    "\n",
    "    dim_red_1 = tokenized_lst\n",
    "\n",
    "    dim_red_2 = stop_words_removed_lst\n",
    "\n",
    "    dim_red_3 = frequency_filtered_lst\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    pos = list(range(n_groups))\n",
    "    bar_width = 0.25\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    rects1 = plt.bar(pos, \n",
    "                     dim_red_1, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     #color='b',\n",
    "                     label='Tokenized (init)')\n",
    "\n",
    "    rects2 = plt.bar([p + bar_width for p in pos], \n",
    "                     dim_red_2, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     #color='b',\n",
    "                     label='Stop Word Removal')\n",
    "\n",
    "    rects3 = plt.bar([p + bar_width*2 for p in pos], \n",
    "                     dim_red_3, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     #color='b',\n",
    "                     label= 'Frequency Filtering')\n",
    "\n",
    "    #plt.xlabel('Group')\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Number of Words')\n",
    "    plt.title('Counts Over Dimensionality Reduction')\n",
    "    plt.xticks([p + + bar_width/2 for p in pos] , ('Total vocab size', \\\n",
    "                                                   'Avg total per book', \\\n",
    "                                                   'Avg unique per book'))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if c_fp != None:\n",
    "        plt.savefig(c_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named matplotlib.pyplot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-36ea3c6fc45d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdim_reduction_chart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-64947646e1a8>\u001b[0m in \u001b[0;36mdim_reduction_chart\u001b[0;34m(d, tokenized_lst, stop_words_removed_lst, frequency_filtered_lst, c_fp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdim_reduction_chart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_lst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_lst\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0mstop_words_removed_lst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_words_removed_lst\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0mfrequency_filtered_lst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrequency_filtered_lst\u001b[0m\u001b[0;34m,\u001b[0m                            \u001b[0mc_fp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mn_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named matplotlib.pyplot"
     ]
    }
   ],
   "source": [
    "dim_reduction_chart(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
