{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, codecs\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THESE SHOULD BE ALL THE RELATIVE PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_dir = '/home/ubuntu/data_download/clean_books/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs_dir = '/home/ubuntu/Capstone/outputs/full_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class IterFile(object):\n",
    "    '''\n",
    "    class object to do the iterating on individual book txt documents, including file i/o.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        \n",
    "    def _open_file(self):\n",
    "        self.file = codecs.open(self.filepath, 'r', encoding='utf_8')\n",
    "        \n",
    "    def _close_file(self):\n",
    "        self.file.close()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        overwrite iteration to include file i/o\n",
    "        '''\n",
    "        self._open_file()\n",
    "        \n",
    "        for line in self.file:\n",
    "            yield line\n",
    "        \n",
    "        self._close_file()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_txt_file_v1(fname, root=source_dir):\n",
    "    '''\n",
    "    Initial pass at text transformation\n",
    "    Reimplemented later (v2 etc) as a caller of various subfunctions to do all the transformation\n",
    "    '''\n",
    "    fp = root + fname\n",
    "\n",
    "    book_as_lst = []\n",
    "    for line in IterFile(fp):\n",
    "        if line == \"\\n\":\n",
    "            pass\n",
    "        else: \n",
    "            line_lst= [tok.lower().strip(punctuation) for tok in line.strip('\\n').split()]\n",
    "            book_as_lst.extend(line_lst)\n",
    "            \n",
    "    #add in stop word removal and frequency threshhold\n",
    "    return book_as_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt',\n",
       " '10.txt',\n",
       " '100.txt',\n",
       " '10000.txt',\n",
       " '10001.txt',\n",
       " '10002-8.txt',\n",
       " '10002.txt',\n",
       " '10003-8.txt',\n",
       " '10003.txt',\n",
       " '10004-8.txt',\n",
       " '10004.txt',\n",
       " '10005-8.txt',\n",
       " '10005.txt',\n",
       " '10006.txt',\n",
       " '10007.txt',\n",
       " '10008.txt',\n",
       " '10009.txt',\n",
       " '1001.txt',\n",
       " '10010.txt',\n",
       " '10011.txt',\n",
       " '10012.txt',\n",
       " '10013.txt',\n",
       " '10014.txt',\n",
       " '10015.txt',\n",
       " '10016.txt',\n",
       " '10017.txt',\n",
       " '10018.txt',\n",
       " '10019.txt',\n",
       " '1002.txt',\n",
       " '10020.txt',\n",
       " '10021.txt',\n",
       " '10022.txt',\n",
       " '10023.txt',\n",
       " '10024.txt',\n",
       " '10025.txt',\n",
       " '10026.txt',\n",
       " '10027.txt',\n",
       " '10028.txt',\n",
       " '10029.txt',\n",
       " '1003.txt',\n",
       " '10030.txt',\n",
       " '10031.txt',\n",
       " '10032.txt',\n",
       " '10033.txt',\n",
       " '10034.txt',\n",
       " '10035.txt',\n",
       " '10036.txt',\n",
       " '10037.txt',\n",
       " '10038.txt',\n",
       " '10039.txt',\n",
       " '1004.txt',\n",
       " '10040.txt',\n",
       " '10041.txt',\n",
       " '10042.txt',\n",
       " '10043.txt',\n",
       " '10044.txt',\n",
       " '10045.txt',\n",
       " '10046.txt',\n",
       " '10047.txt',\n",
       " '10048.txt',\n",
       " '10049.txt',\n",
       " '1005.txt',\n",
       " '10050.txt',\n",
       " '10051.txt',\n",
       " '10052.txt',\n",
       " '10056.txt',\n",
       " '10057.txt',\n",
       " '10058.txt',\n",
       " '10059.txt',\n",
       " '1006.txt',\n",
       " '10060.txt',\n",
       " '10062.txt',\n",
       " '10063.txt',\n",
       " '10064.txt',\n",
       " '10065.txt',\n",
       " '10066.txt',\n",
       " '10067.txt',\n",
       " '10068.txt',\n",
       " '10069.txt',\n",
       " '1007.txt',\n",
       " '10070.txt',\n",
       " '10071.txt',\n",
       " '10072.txt',\n",
       " '10073.txt',\n",
       " '10074.txt',\n",
       " '10075.txt',\n",
       " '10076.txt',\n",
       " '10077.txt',\n",
       " '10078.txt',\n",
       " '10079.txt',\n",
       " '1008.txt',\n",
       " '10080.txt',\n",
       " '10081.txt',\n",
       " '10082.txt',\n",
       " '10083.txt',\n",
       " '10084.txt',\n",
       " '10085.txt',\n",
       " '10086.txt',\n",
       " '10087.txt',\n",
       " '10088.txt',\n",
       " '10089.txt',\n",
       " '10090.txt',\n",
       " '10091.txt',\n",
       " '10092.txt',\n",
       " '10093.txt',\n",
       " '10094.txt',\n",
       " '10095.txt',\n",
       " '10096.txt',\n",
       " '10097.txt',\n",
       " '10098.txt',\n",
       " '10099.txt',\n",
       " '101.txt',\n",
       " '10100.txt',\n",
       " '10101.txt',\n",
       " '10102.txt',\n",
       " '10103.txt',\n",
       " '10104.txt',\n",
       " '10105.txt',\n",
       " '10106.txt',\n",
       " '10107.txt',\n",
       " '10108.txt',\n",
       " '10109.txt',\n",
       " '10110.txt',\n",
       " '10111.txt',\n",
       " '10112.txt',\n",
       " '10114.txt',\n",
       " '10116.txt',\n",
       " '10118.txt',\n",
       " '10119.txt',\n",
       " '10120.txt',\n",
       " '10121.txt',\n",
       " '10122.txt',\n",
       " '10123.txt',\n",
       " '10124.txt',\n",
       " '10125.txt',\n",
       " '10126.txt',\n",
       " '10127.txt',\n",
       " '10128.txt',\n",
       " '10129.txt',\n",
       " '1013.txt',\n",
       " '10130.txt',\n",
       " '10131.txt',\n",
       " '10132.txt',\n",
       " '10133.txt',\n",
       " '10134.txt',\n",
       " '10135.txt',\n",
       " '10136.txt',\n",
       " '10138.txt',\n",
       " '10139.txt',\n",
       " '1014.txt',\n",
       " '10141.txt',\n",
       " '10142.txt',\n",
       " '10143.txt',\n",
       " '10144.txt',\n",
       " '10145.txt',\n",
       " '10146.txt',\n",
       " '10147.txt',\n",
       " '10148.txt',\n",
       " '10149.txt',\n",
       " '1015.txt',\n",
       " '10150.txt',\n",
       " '10151.txt',\n",
       " '10159.txt',\n",
       " '1016.txt',\n",
       " '10161.txt',\n",
       " '10162.txt',\n",
       " '10163.txt',\n",
       " '10164.txt',\n",
       " '10165.txt',\n",
       " '10166.txt',\n",
       " '1019.txt',\n",
       " '102.txt',\n",
       " '1020.txt',\n",
       " '10201.txt',\n",
       " '10202.txt',\n",
       " '1021.txt',\n",
       " '10210.txt',\n",
       " '10211.txt',\n",
       " '10212.txt',\n",
       " '10213.txt',\n",
       " '10214.txt',\n",
       " '10216.txt',\n",
       " '10217.txt',\n",
       " '10219.txt',\n",
       " '10220.txt',\n",
       " '10221.txt',\n",
       " '10222.txt',\n",
       " '10224.txt',\n",
       " '10225.txt',\n",
       " '10226.txt',\n",
       " '1023.txt',\n",
       " '10234.txt',\n",
       " '1024.txt',\n",
       " '1025.txt',\n",
       " '10251.txt',\n",
       " '10266.txt',\n",
       " '10267.txt',\n",
       " '10268.txt',\n",
       " '1027.txt',\n",
       " '10274.txt',\n",
       " '1028.txt',\n",
       " '1029.txt',\n",
       " '10291.txt',\n",
       " '10292.txt',\n",
       " '10294.txt',\n",
       " '103.txt',\n",
       " '10310.txt',\n",
       " '10314.txt',\n",
       " '10315.txt',\n",
       " '10316.txt',\n",
       " '10317.txt',\n",
       " '10318.txt',\n",
       " '10319.txt',\n",
       " '1032.txt',\n",
       " '10320.txt',\n",
       " '10321.txt',\n",
       " '10322.txt',\n",
       " '10323.txt',\n",
       " '10324.txt',\n",
       " '10325.txt',\n",
       " '10326.txt',\n",
       " '10327.txt',\n",
       " '10328.txt',\n",
       " '10329.txt',\n",
       " '1033.txt',\n",
       " '10330.txt',\n",
       " '10331.txt',\n",
       " '10332.txt',\n",
       " '10333.txt',\n",
       " '10335.txt',\n",
       " '10336.txt',\n",
       " '10337.txt',\n",
       " '10338.txt',\n",
       " '1034.txt',\n",
       " '10340.txt',\n",
       " '10341.txt',\n",
       " '10342.txt',\n",
       " '10343.txt',\n",
       " '10345.txt',\n",
       " '10348.txt',\n",
       " '1035.txt',\n",
       " '10350.txt',\n",
       " '10351.txt',\n",
       " '10352.txt',\n",
       " '10355.txt',\n",
       " '10356.txt',\n",
       " '10357.txt',\n",
       " '10358.txt',\n",
       " '10359.txt',\n",
       " '1036.txt',\n",
       " '10360.txt',\n",
       " '10361.txt',\n",
       " '10362.txt',\n",
       " '10363.txt',\n",
       " '10364.txt',\n",
       " '10366.txt',\n",
       " '10367.txt',\n",
       " '10368.txt',\n",
       " '10369.txt',\n",
       " '1037.txt',\n",
       " '10370.txt',\n",
       " '10371.txt',\n",
       " '10372.txt',\n",
       " '10373.txt',\n",
       " '10374.txt',\n",
       " '10375.txt',\n",
       " '10377.txt',\n",
       " '10378.txt',\n",
       " '10379.txt',\n",
       " '10380.txt',\n",
       " '10381.txt',\n",
       " '10382.txt',\n",
       " '10383.txt',\n",
       " '10386.txt',\n",
       " '10387.txt',\n",
       " '10388.txt',\n",
       " '10389.txt',\n",
       " '1039.txt',\n",
       " '10390.txt',\n",
       " '10391.txt',\n",
       " '10392.txt',\n",
       " '10393.txt',\n",
       " '10394.txt',\n",
       " '10395.txt',\n",
       " '10396.txt',\n",
       " '10397.txt',\n",
       " '10398.txt',\n",
       " '104.txt',\n",
       " '1040.txt',\n",
       " '10401.txt',\n",
       " '10402.txt',\n",
       " '10403.txt',\n",
       " '10404.txt',\n",
       " '10409.txt',\n",
       " '1041.txt',\n",
       " '10410.txt',\n",
       " '10417.txt',\n",
       " '10418.txt',\n",
       " '10419.txt',\n",
       " '10420.txt',\n",
       " '10421.txt',\n",
       " '10422.txt',\n",
       " '10427.txt',\n",
       " '10429.txt',\n",
       " '1043.txt',\n",
       " '10430.txt',\n",
       " '10431.txt',\n",
       " '10432.txt',\n",
       " '10433.txt',\n",
       " '10434.txt',\n",
       " '10435.txt',\n",
       " '10436.txt',\n",
       " '10437.txt',\n",
       " '10438.txt',\n",
       " '10439.txt',\n",
       " '10440.txt',\n",
       " '10441.txt',\n",
       " '10443.txt',\n",
       " '10444.txt',\n",
       " '10445.txt',\n",
       " '10446.txt',\n",
       " '10447.txt',\n",
       " '10448.txt',\n",
       " '10449.txt',\n",
       " '1045.txt',\n",
       " '10450.txt',\n",
       " '10451.txt',\n",
       " '10452.txt',\n",
       " '10453.txt',\n",
       " '10454.txt',\n",
       " '10455.txt',\n",
       " '10456.txt',\n",
       " '10457.txt',\n",
       " '10458.txt',\n",
       " '10459.txt',\n",
       " '1046.txt',\n",
       " '10460.txt',\n",
       " '10461.txt',\n",
       " '10463.txt',\n",
       " '10464.txt',\n",
       " '10465.txt',\n",
       " '10466.txt',\n",
       " '10467.txt',\n",
       " '10468.txt',\n",
       " '10469.txt',\n",
       " '1047.txt',\n",
       " '10470.txt',\n",
       " '10471.txt',\n",
       " '10472.txt',\n",
       " '10473.txt',\n",
       " '10474.txt',\n",
       " '10475.txt',\n",
       " '10476.txt',\n",
       " '10477.txt',\n",
       " '10478.txt',\n",
       " '10479.txt',\n",
       " '1048.txt',\n",
       " '10482.txt',\n",
       " '10483.txt',\n",
       " '10484.txt',\n",
       " '10485.txt',\n",
       " '10486.txt',\n",
       " '10487.txt',\n",
       " '10488.txt',\n",
       " '10489.txt',\n",
       " '1049.txt',\n",
       " '10490.txt',\n",
       " '10491.txt',\n",
       " '10494.txt',\n",
       " '10495.txt',\n",
       " '10496.txt',\n",
       " '10497.txt',\n",
       " '10498.txt',\n",
       " '10499.txt',\n",
       " '105.txt',\n",
       " '1050.txt',\n",
       " '10500.txt',\n",
       " '10501.txt',\n",
       " '10503.txt',\n",
       " '10504.txt',\n",
       " '10505.txt',\n",
       " '10508.txt',\n",
       " '10509.txt',\n",
       " '1051.txt',\n",
       " '10510.txt',\n",
       " '10511.txt',\n",
       " '10513.txt',\n",
       " '10515.txt',\n",
       " '10516.txt',\n",
       " '10517.txt',\n",
       " '10518.txt',\n",
       " '10519.txt',\n",
       " '1052.txt',\n",
       " '10520.txt',\n",
       " '10521.txt',\n",
       " '10522.txt',\n",
       " '10523.txt',\n",
       " '10524.txt',\n",
       " '10525.txt',\n",
       " '10526.txt',\n",
       " '10527.txt',\n",
       " '10528.txt',\n",
       " '10529.txt',\n",
       " '1053.txt',\n",
       " '10530.txt',\n",
       " '10531.txt',\n",
       " '10532.txt',\n",
       " '10533.txt',\n",
       " '10534.txt',\n",
       " '10535.txt',\n",
       " '10537.txt',\n",
       " '10538.txt',\n",
       " '10539.txt',\n",
       " '10540.txt',\n",
       " '10541.txt',\n",
       " '10542.txt',\n",
       " '10543.txt',\n",
       " '10544.txt',\n",
       " '10545.txt',\n",
       " '10546.txt',\n",
       " '10548.txt',\n",
       " '10549.txt',\n",
       " '10550.txt',\n",
       " '10551.txt',\n",
       " '10552.txt',\n",
       " '10554.txt',\n",
       " '10555.txt',\n",
       " '10556.txt',\n",
       " '10557.txt',\n",
       " '1056.txt',\n",
       " '10560.txt',\n",
       " '10561.txt',\n",
       " '10562.txt',\n",
       " '10563.txt',\n",
       " '10564.txt',\n",
       " '10565.txt',\n",
       " '10566.txt',\n",
       " '10567.txt',\n",
       " '10568.txt',\n",
       " '10569.txt',\n",
       " '10570.txt',\n",
       " '10571.txt',\n",
       " '10572.txt',\n",
       " '10573.txt',\n",
       " '10574.txt',\n",
       " '10575.txt',\n",
       " '10576.txt',\n",
       " '10577.txt',\n",
       " '10578.txt',\n",
       " '10579.txt',\n",
       " '10580.txt',\n",
       " '10581.txt',\n",
       " '10582.txt',\n",
       " '10583.txt',\n",
       " '10584.txt',\n",
       " '10585.txt',\n",
       " '10586.txt',\n",
       " '10587.txt',\n",
       " '10588.txt',\n",
       " '10589.txt',\n",
       " '1059.txt',\n",
       " '10590.txt',\n",
       " '10591.txt',\n",
       " '10592.txt',\n",
       " '10593.txt',\n",
       " '10594.txt',\n",
       " '10595.txt',\n",
       " '10596.txt',\n",
       " '10597.txt',\n",
       " '10598.txt',\n",
       " '10599.txt',\n",
       " '106.txt',\n",
       " '10600.txt',\n",
       " '10601.txt',\n",
       " '10602.txt',\n",
       " '10603.txt',\n",
       " '10605.txt',\n",
       " '10606.txt',\n",
       " '10607.txt',\n",
       " '10608.txt',\n",
       " '10609.txt',\n",
       " '1061.txt',\n",
       " '10610.txt',\n",
       " '10611.txt',\n",
       " '10612.txt',\n",
       " '10613.txt',\n",
       " '10614.txt',\n",
       " '10616.txt',\n",
       " '10617.txt',\n",
       " '10618.txt',\n",
       " '10619.txt',\n",
       " '1062.txt',\n",
       " '10620.txt',\n",
       " '10621.txt',\n",
       " '10622.txt',\n",
       " '10623.txt',\n",
       " '10624.txt',\n",
       " '10626.txt',\n",
       " '10627.txt',\n",
       " '10628.txt',\n",
       " '10629.txt',\n",
       " '1063.txt',\n",
       " '10630.txt',\n",
       " '10631.txt',\n",
       " '10632.txt',\n",
       " '10633.txt',\n",
       " '10635.txt',\n",
       " '10636.txt',\n",
       " '10637.txt',\n",
       " '10638.txt',\n",
       " '10639.txt',\n",
       " '1064.txt',\n",
       " '10640.txt',\n",
       " '10641.txt',\n",
       " '10642.txt',\n",
       " '10643.txt',\n",
       " '10644.txt',\n",
       " '10645.txt',\n",
       " '10646.txt',\n",
       " '10647.txt',\n",
       " '10648.txt',\n",
       " '10649.txt',\n",
       " '1065.txt',\n",
       " '10650.txt',\n",
       " '10651.txt',\n",
       " '10652.txt',\n",
       " '10653.txt',\n",
       " '10654.txt',\n",
       " '10655.txt',\n",
       " '10656.txt',\n",
       " '10657.txt',\n",
       " '10658.txt',\n",
       " '10659.txt',\n",
       " '10660.txt',\n",
       " '10661.txt',\n",
       " '10662.txt',\n",
       " '10663.txt',\n",
       " '10665.txt',\n",
       " '10666.txt',\n",
       " '10667.txt',\n",
       " '10668.txt',\n",
       " '10669.txt',\n",
       " '1067.txt',\n",
       " '10670.txt',\n",
       " '10671.txt',\n",
       " '10672.txt',\n",
       " '10673.txt',\n",
       " '10674.txt',\n",
       " '10675.txt',\n",
       " '10676.txt',\n",
       " '10677.txt',\n",
       " '10679.txt',\n",
       " '1068.txt',\n",
       " '10684.txt',\n",
       " '10688.txt',\n",
       " '1069.txt',\n",
       " '10690.txt',\n",
       " '10691.txt',\n",
       " '10692.txt',\n",
       " '10693.txt',\n",
       " '10694.txt',\n",
       " '10695.txt',\n",
       " '10696.txt',\n",
       " '10698.txt',\n",
       " '10699.txt',\n",
       " '107.txt',\n",
       " '10700.txt',\n",
       " '10701.txt',\n",
       " '10702.txt',\n",
       " '10703.txt',\n",
       " '10704.txt',\n",
       " '10705.txt',\n",
       " '10706.txt',\n",
       " '10707.txt',\n",
       " '10708.txt',\n",
       " '10709.txt',\n",
       " '10710.txt',\n",
       " '10711.txt',\n",
       " '10712.txt',\n",
       " '10713.txt',\n",
       " '10714.txt',\n",
       " '10715.txt',\n",
       " '10716.txt',\n",
       " '10717.txt',\n",
       " '10718.txt',\n",
       " '10719.txt',\n",
       " '10720.txt',\n",
       " '10721.txt',\n",
       " '10722.txt',\n",
       " '10723.txt',\n",
       " '10724.txt',\n",
       " '10725.txt',\n",
       " '10726.txt',\n",
       " '10727.txt',\n",
       " '10728.txt',\n",
       " '10729.txt',\n",
       " '10730.txt',\n",
       " '10731.txt',\n",
       " '10732.txt',\n",
       " '10733.txt',\n",
       " '10734.txt',\n",
       " '10735.txt',\n",
       " '10736.txt',\n",
       " '10737.txt',\n",
       " '10738.txt',\n",
       " '10739.txt',\n",
       " '10740.txt',\n",
       " '10741.txt',\n",
       " '10742.txt',\n",
       " '10743.txt',\n",
       " '10744.txt',\n",
       " '10745.txt',\n",
       " '10747.txt',\n",
       " '10748.txt',\n",
       " '10749.txt',\n",
       " '10750.txt',\n",
       " '10751.txt',\n",
       " '10753.txt',\n",
       " '10754.txt',\n",
       " '10755.txt',\n",
       " '10756.txt',\n",
       " '10757.txt',\n",
       " '10758.txt',\n",
       " '10759.txt',\n",
       " '1076.txt',\n",
       " '10760.txt',\n",
       " '10761.txt',\n",
       " '10762.txt',\n",
       " '10763.txt',\n",
       " '10765.txt',\n",
       " '10766.txt',\n",
       " '10767.txt',\n",
       " '10769.txt',\n",
       " '1077.txt',\n",
       " '10770.txt',\n",
       " '10771.txt',\n",
       " '10772.txt',\n",
       " '10773.txt',\n",
       " '10776.txt',\n",
       " '10777.txt',\n",
       " '10778.txt',\n",
       " '10779.txt',\n",
       " '1078.txt',\n",
       " '10780.txt',\n",
       " '10781.txt',\n",
       " '10782.txt',\n",
       " '10783.txt',\n",
       " '10784.txt',\n",
       " '10785.txt',\n",
       " '10786.txt',\n",
       " '10787.txt',\n",
       " '10788.txt',\n",
       " '10789.txt',\n",
       " '1079.txt',\n",
       " '10790.txt',\n",
       " '10791.txt',\n",
       " '10792.txt',\n",
       " '10794.txt',\n",
       " '10795.txt',\n",
       " '10796.txt',\n",
       " '10797.txt',\n",
       " '10798.txt',\n",
       " '10799.txt',\n",
       " '108.txt',\n",
       " '1080.txt',\n",
       " '10800.txt',\n",
       " '10801.txt',\n",
       " '10803.txt',\n",
       " '10804.txt',\n",
       " '10805.txt',\n",
       " '10806.txt',\n",
       " '10807.txt',\n",
       " '10808.txt',\n",
       " '10809.txt',\n",
       " '1081.txt',\n",
       " '10810.txt',\n",
       " '10811.txt',\n",
       " '10812.txt',\n",
       " '10813.txt',\n",
       " '10814.txt',\n",
       " '10815.txt',\n",
       " '10816.txt',\n",
       " '10817.txt',\n",
       " '10818.txt',\n",
       " '1082.txt',\n",
       " '10826.txt',\n",
       " '10827.txt',\n",
       " '10828.txt',\n",
       " '1083.txt',\n",
       " '10830.txt',\n",
       " '10831.txt',\n",
       " '10832.txt',\n",
       " '10833.txt',\n",
       " '10834.txt',\n",
       " '10835.txt',\n",
       " '10837.txt',\n",
       " '10838.txt',\n",
       " '10839.txt',\n",
       " '1084.txt',\n",
       " '10840.txt',\n",
       " '10842.txt',\n",
       " '10843.txt',\n",
       " '10844.txt',\n",
       " '10845.txt',\n",
       " '10846.txt',\n",
       " '10847.txt',\n",
       " '10848.txt',\n",
       " '10849.txt',\n",
       " '1085.txt',\n",
       " '10850.txt',\n",
       " '10851.txt',\n",
       " '10852.txt',\n",
       " '10853.txt',\n",
       " '10854.txt',\n",
       " '10855.txt',\n",
       " '10856.txt',\n",
       " '10857.txt',\n",
       " '10858.txt',\n",
       " '10859.txt',\n",
       " '10860.txt',\n",
       " '10861.txt',\n",
       " '10862.txt',\n",
       " '10864.txt',\n",
       " '10865.txt',\n",
       " '10866.txt',\n",
       " '10867.txt',\n",
       " '10868.txt',\n",
       " '10869.txt',\n",
       " '1087.txt',\n",
       " '10870.txt',\n",
       " '10871.txt',\n",
       " '10872.txt',\n",
       " '10873.txt',\n",
       " '10874.txt',\n",
       " '10875.txt',\n",
       " '10876.txt',\n",
       " '10877.txt',\n",
       " '10878.txt',\n",
       " '10879.txt',\n",
       " '1088.txt',\n",
       " '10880.txt',\n",
       " '10881.txt',\n",
       " '10882.txt',\n",
       " '10883.txt',\n",
       " '10884.txt',\n",
       " '10885.txt',\n",
       " '10886.txt',\n",
       " '10887.txt',\n",
       " '10888.txt',\n",
       " '10889.txt',\n",
       " '1089.txt',\n",
       " '10890.txt',\n",
       " '10891.txt',\n",
       " '10892.txt',\n",
       " '10893.txt',\n",
       " '10894.txt',\n",
       " '10895.txt',\n",
       " '10896.txt',\n",
       " '10897.txt',\n",
       " '10898.txt',\n",
       " '10899.txt',\n",
       " '109.txt',\n",
       " '1090.txt',\n",
       " '10900.txt',\n",
       " '10901.txt',\n",
       " '10902.txt',\n",
       " '10903.txt',\n",
       " '10904.txt',\n",
       " '10905.txt',\n",
       " '10907.txt',\n",
       " '10908.txt',\n",
       " '1091.txt',\n",
       " '10910.txt',\n",
       " '10911.txt',\n",
       " '10912.txt',\n",
       " '10913.txt',\n",
       " '10915.txt',\n",
       " '10916.txt',\n",
       " '10918.txt',\n",
       " '10919.txt',\n",
       " '10920.txt',\n",
       " '10921.txt',\n",
       " '10922.txt',\n",
       " '10923.txt',\n",
       " '10924.txt',\n",
       " '10925.txt',\n",
       " '10926.txt',\n",
       " '10928.txt',\n",
       " '10929.txt',\n",
       " '1093.txt',\n",
       " '10930.txt',\n",
       " '10931.txt',\n",
       " '10932.txt',\n",
       " '10933.txt',\n",
       " '10934.txt',\n",
       " '10935.txt',\n",
       " '10936.txt',\n",
       " '10937.txt',\n",
       " '10938.txt',\n",
       " '10939.txt',\n",
       " '1094.txt',\n",
       " '10940.txt',\n",
       " '10942.txt',\n",
       " '10943.txt',\n",
       " '10944.txt',\n",
       " '10945.txt',\n",
       " '10946.txt',\n",
       " '10947.txt',\n",
       " '10948.txt',\n",
       " '10949.txt',\n",
       " '1095.txt',\n",
       " '10950.txt',\n",
       " '10951.txt',\n",
       " '10952.txt',\n",
       " '10954.txt',\n",
       " '10955.txt',\n",
       " '10956.txt',\n",
       " '10957.txt',\n",
       " '10958.txt',\n",
       " '10959.txt',\n",
       " '1096.txt',\n",
       " '10960.txt',\n",
       " '10961.txt',\n",
       " '10962.txt',\n",
       " '10963.txt',\n",
       " '10964.txt',\n",
       " '10965.txt',\n",
       " '10966.txt',\n",
       " '10967.txt',\n",
       " '10968.txt',\n",
       " '10969.txt',\n",
       " '1097.txt',\n",
       " '10970.txt',\n",
       " '10972.txt',\n",
       " '10973.txt',\n",
       " '10974.txt',\n",
       " '10976.txt',\n",
       " '10977.txt',\n",
       " '10978.txt',\n",
       " '10979.txt',\n",
       " '1098.txt',\n",
       " '10980.txt',\n",
       " '10981.txt',\n",
       " '10983.txt',\n",
       " '10984.txt',\n",
       " '10985.txt',\n",
       " '10986.txt',\n",
       " '10987.txt',\n",
       " '10988.txt',\n",
       " '10989.txt',\n",
       " '1099.txt',\n",
       " '10990.txt',\n",
       " '10991.txt',\n",
       " '10992.txt',\n",
       " '10993.txt',\n",
       " '10994.txt',\n",
       " '10995.txt',\n",
       " '10996.txt',\n",
       " '10997.txt',\n",
       " '10998.txt',\n",
       " '10999.txt',\n",
       " '11.txt',\n",
       " '110.txt',\n",
       " '1100.txt',\n",
       " '11003.txt',\n",
       " '11004.txt',\n",
       " '11005.txt',\n",
       " '11006.txt',\n",
       " '11007.txt',\n",
       " '11008.txt',\n",
       " '11009.txt',\n",
       " '1101.txt',\n",
       " '11010.txt',\n",
       " '11011.txt',\n",
       " '11012.txt',\n",
       " '11013.txt',\n",
       " '11014.txt',\n",
       " '11015.txt',\n",
       " '11016.txt',\n",
       " '11017.txt',\n",
       " '11018.txt',\n",
       " '11019.txt',\n",
       " '1102.txt',\n",
       " '11020.txt',\n",
       " '11021.txt',\n",
       " '11022.txt',\n",
       " '11023.txt',\n",
       " '11025.txt',\n",
       " '11026.txt',\n",
       " '11027.txt',\n",
       " '11028.txt',\n",
       " '11029.txt',\n",
       " '1103.txt',\n",
       " '11030.txt',\n",
       " '11031.txt',\n",
       " '11032.txt',\n",
       " '11033.txt',\n",
       " '11034.txt',\n",
       " '11039.txt',\n",
       " '1104.txt',\n",
       " '11041.txt',\n",
       " '11043.txt',\n",
       " '11044.txt',\n",
       " '11045.txt',\n",
       " '11047.txt',\n",
       " '1105.txt',\n",
       " '11050.txt',\n",
       " '11051.txt',\n",
       " '11052.txt',\n",
       " '11053.txt',\n",
       " '11054.txt',\n",
       " '11055.txt',\n",
       " '11056.txt',\n",
       " '11057.txt',\n",
       " '11058.txt',\n",
       " '11059.txt',\n",
       " '1106.txt',\n",
       " '11060.txt',\n",
       " '11061.txt',\n",
       " '11062.txt',\n",
       " '11063.txt',\n",
       " '11064.txt',\n",
       " '11065.txt',\n",
       " '11066.txt',\n",
       " '11067.txt',\n",
       " '11068.txt',\n",
       " '11069.txt',\n",
       " '1107.txt',\n",
       " '11073.txt',\n",
       " '11074.txt',\n",
       " '11076.txt',\n",
       " '11077.txt',\n",
       " '11078.txt',\n",
       " '11079.txt',\n",
       " '1108.txt',\n",
       " '11080.txt',\n",
       " '11082.txt',\n",
       " '11083.txt',\n",
       " '11084.txt',\n",
       " '11085.txt',\n",
       " '11086.txt',\n",
       " '11087.txt',\n",
       " '11088.txt',\n",
       " '11089.txt',\n",
       " '1109.txt',\n",
       " '11090.txt',\n",
       " '11091.txt',\n",
       " '11092.txt',\n",
       " '11093.txt',\n",
       " '11094.txt',\n",
       " '11095.txt',\n",
       " '11096.txt',\n",
       " '11097.txt',\n",
       " '11098.txt',\n",
       " '11099.txt',\n",
       " '111.txt',\n",
       " '1110.txt',\n",
       " '11100.txt',\n",
       " '11101.txt',\n",
       " '11102.txt',\n",
       " '11103.txt',\n",
       " '11104.txt',\n",
       " '11105.txt',\n",
       " '11106.txt',\n",
       " '11107.txt',\n",
       " '11109.txt',\n",
       " '1111.txt',\n",
       " '11110.txt',\n",
       " '11111.txt',\n",
       " '11112.txt',\n",
       " '11113.txt',\n",
       " '11114.txt',\n",
       " '11115.txt',\n",
       " '11116.txt',\n",
       " '11117.txt',\n",
       " '11118.txt',\n",
       " '11119.txt',\n",
       " '1112.txt',\n",
       " '11120.txt',\n",
       " '11121.txt',\n",
       " '11122.txt',\n",
       " '11123.txt',\n",
       " '11124.txt',\n",
       " '11125.txt',\n",
       " '11126.txt',\n",
       " '11127.txt',\n",
       " '11128.txt',\n",
       " '11129.txt',\n",
       " '1113.txt',\n",
       " '11133.txt',\n",
       " '11134.txt',\n",
       " '11135.txt',\n",
       " '11136.txt',\n",
       " '11137.txt',\n",
       " '11138.txt',\n",
       " '1114.txt',\n",
       " '11140.txt',\n",
       " '11141.txt',\n",
       " '11142.txt',\n",
       " '11143.txt',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_corp = PlaintextCorpusReader(source_dir, '.*\\.txt')\n",
    "fileid_lst = temp_corp.fileids()\n",
    "fileid_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Saving helper funtions\n",
    "\n",
    "To automate EDA steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eda(transform_txt_file, fileid_lst=fileid_lst):\n",
    "    '''\n",
    "    Do transformations with updated transformation function and return all the eda items\n",
    "    '''\n",
    "    \n",
    "    all_transf_books_lst = [transform_txt_file(f) for f in fileid_lst]\n",
    "    \n",
    "    book_lengths = [(tup[0], len(tup[1])) for tup in zip(fileid_lst, all_transf_books_lst)]\n",
    "    avg_num_tokens = int(np.mean([len(book) for book in all_transf_books_lst]))\n",
    "    \n",
    "    dictionary = corpora.Dictionary(all_transf_books_lst)\n",
    "    dictionary_length = len(dictionary)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(book) for book in all_transf_books_lst]\n",
    "    \n",
    "    unique_toks_num_lst = [len(book) for book in corpus]\n",
    "    unique_toks_per_fileid = zip(fileid_lst, unique_toks_num_lst)\n",
    "    avg_unique_toks = int(np.mean(unique_toks_num_lst))\n",
    "    \n",
    "    \n",
    "    return book_lengths, avg_num_tokens, dictionary, dictionary_length, unique_toks_per_fileid, avg_unique_toks, corpus\n",
    "\n",
    "\n",
    "def save_stuff(distinguishing_str, dictionary, corpus, outputs_dir=outputs_dir):\n",
    "    '''\n",
    "    Save the outputs of the most recent eda step\n",
    "    '''\n",
    "    dictionary.save(outputs_dir + distinguishing_str + '.dict')\n",
    "    corpora.MmCorpus.serialize(outputs_dir + distinguishing_str + '_corpus.mm', corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA items\n",
    "* List of book lengths (total num of tokens for each book)\n",
    "* Average number of tokens per book\n",
    "* Number of words in corpus (dictionary length)\n",
    "    * Dictionary (not viewed)\n",
    "* Unique tokens per book\n",
    "* Average number of unique tokens per book\n",
    "    * Corpus (not viewe)\n",
    "    \n",
    "Save everything after eda step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To summarize the 'simple tokenization' EDA step (#1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_v1 = eda(transform_txt_file_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_lengths1, avg_num_tokens1, dictionary1, dictionary_length1, unique_toks_per_fileid1, \\\n",
    "    avg_unique_toks1, corpus1 = output_v1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Average number of tokens in a book: \", avg_num_tokens1\n",
    "print \"   \"\n",
    "print \"Average unique tokens in a book: \", avg_unique_toks1\n",
    "print \"   \"\n",
    "print \"Total number of words (dictionary length): \", dictionary_length1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##for pres, note the sparcity problem - 9000 vs. 195k == 186k empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_stuff('simple_tok', dictionary1, corpus1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Step 2: + stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "print stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_txt_file_v2(fname, root=source_dir, stop_words=stop):\n",
    "    '''\n",
    "    Top-level function to call all of the subfunctions for text transformation\n",
    "    Assumes you want to remove empty lines and tokenize (because you do)\n",
    "    '''\n",
    "    fp = root + fname\n",
    "    book_as_lst = []\n",
    "    for line in IterFile(fp):\n",
    "        \n",
    "        if empty_line_check(line) == False:\n",
    "            line = basic_tokenize(line)\n",
    "            \n",
    "            if stop_words !=None:\n",
    "                line = remove_stop_words(line, stop_words)\n",
    "        \n",
    "        book_as_lst.extend(line)\n",
    "        \n",
    "    return book_as_lst\n",
    "\n",
    "def empty_line_check(line) :\n",
    "    '''\n",
    "    checks for empty line\n",
    "    '''\n",
    "    if line == \"\\n\":\n",
    "        empty = True\n",
    "    else:\n",
    "        empty = False\n",
    "    return empty\n",
    "    \n",
    "def basic_tokenize(line):\n",
    "    '''\n",
    "    convert to list\n",
    "    strip punctuation, lowercase\n",
    "    '''\n",
    "    return [tok.lower().strip(punctuation) for tok in line.strip('\\n').split()]    \n",
    "            \n",
    "def remove_stop_words(line, stop_words):\n",
    "    return [tok for tok in line if tok not in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_v2 = eda(transform_txt_file_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "book_lengths2, avg_num_tokens2, dictionary2, dictionary_length2, \\\n",
    "    unique_toks_per_fileid2, avg_unique_toks2, corpus2 = output_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Average number of tokens in a book: \", avg_num_tokens2\n",
    "print \"   \"\n",
    "print \"Average unique tokens in a book: \", avg_unique_toks2\n",
    "print \"   \"\n",
    "print \"Total number of words (dictionary length): \", dictionary_length2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make graph of reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_stuff('no_stopwords', dictionary2, corpus2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Step 4: + frequency filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eda_w_filter(transform_txt_file, fileid_lst=fileid_lst):\n",
    "    '''\n",
    "    Do transformations with updated transformation function and return all the eda items\n",
    "    '''\n",
    "    \n",
    "    all_transf_books_lst = [transform_txt_file(f) for f in fileid_lst]\n",
    "    \n",
    "    book_lengths = [(tup[0], len(tup[1])) for tup in zip(fileid_lst, all_transf_books_lst)]\n",
    "    avg_num_tokens = int(np.mean([len(book) for book in all_transf_books_lst]))\n",
    "    \n",
    "    dictionary = corpora.Dictionary(all_transf_books_lst)\n",
    "    dictionary.filter_extremes(no_below=1)\n",
    "    dictionary_length = len(dictionary)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(book) for book in all_transf_books_lst]\n",
    "    \n",
    "    unique_toks_num_lst = [len(book) for book in corpus]\n",
    "    unique_toks_per_fileid = zip(fileid_lst, unique_toks_num_lst)\n",
    "    avg_unique_toks = int(np.mean(unique_toks_num_lst))\n",
    "    \n",
    "    \n",
    "    return book_lengths, avg_num_tokens, dictionary, dictionary_length, unique_toks_per_fileid, avg_unique_toks, corpus\n",
    "\n",
    "\n",
    "def save_stuff(distinguishing_str, dictionary, corpus, model, outputs_dir=outputs_dir):\n",
    "    '''\n",
    "    Save the outputs of the most recent eda step\n",
    "    '''\n",
    "    if dictionary != None:\n",
    "        dictionary.save(outputs_dir + distinguishing_str + '.dict')\n",
    "        \n",
    "    if corpus != None:\n",
    "        corpora.MmCorpus.serialize(outputs_dir + distinguishing_str + '_corpus.mm', corpus)\n",
    "    \n",
    "    if model != None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs4 = eda_w_filter(transform_txt_file_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "book_lengths4, avg_num_tokens4, dictionary4, dictionary_length4, \\\n",
    "    unique_toks_per_fileid4, avg_unique_toks4, corpus4 = outputs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Average number of tokens in a book: \", avg_num_tokens4, avg_num_tokens2\n",
    "print \"   \"\n",
    "print \"Average unique tokens in a book: \", avg_unique_toks4, avg_unique_toks2\n",
    "print \"   \"\n",
    "print \"Total number of words (dictionary length): \", dictionary_length4, dictionary_length2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_stuff('frequency_filtered', dictionary4, corpus4, model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_lengths4, avg_num_tokens4, dictionary4, dictionary_length4, \\\n",
    "    unique_toks_per_fileid4, avg_unique_toks4, corpus4 = outputs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Average unique words per book: \"\n",
    "print \"   \", \"Initial (tokenized): \", avg_num_tokens1\n",
    "print \"   \", \"Stop words removed: \", avg_num_tokens2\n",
    "print \"   \", \"Frequency filtered: \", avg_num_tokens4\n",
    "print \"   \"\n",
    "print \"Average unique words per book: \"\n",
    "print \"   \", \"Initial (tokenized): \", avg_unique_toks1\n",
    "print \"   \", \"Stop words removed: \", avg_unique_toks2\n",
    "print \"   \", \"Frequency filtered: \", avg_unique_toks4\n",
    "print \"   \"\n",
    "print \"Vocabulary length: \"\n",
    "print \"   \", \"Initial (tokenized): \", dictionary_length1\n",
    "print \"   \", \"Stop words removed: \", dictionary_length2\n",
    "print \"   \", \"Frequency filtered: \", dictionary_length4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_num_tokens_lst = [avg_num_tokens1, avg_num_tokens2, avg_num_tokens4]\n",
    "avg_unique_toks_lst =  [avg_unique_toks1, avg_unique_toks2, avg_unique_toks4]\n",
    "vocab_size_lst =[dictionary_length1, dictionary_length2, dictionary_length4]\n",
    "avg_num_tokens_lst, avg_unique_toks_lst, vocab_size_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_lst = [avg_unique_toks1, avg_num_tokens1, dictionary_length1]\n",
    "stop_words_removed_lst =  [avg_unique_toks2, avg_num_tokens2, dictionary_length2]\n",
    "frequency_filtered_lst =[avg_unique_toks4, avg_num_tokens4, dictionary_length4]\n",
    "tokenized_lst, stop_words_removed_lst, frequency_filtered_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#%matplotinline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_groups = 3\n",
    "pos = list(range(n_groups))\n",
    "\n",
    "dim_red_1 = tokenized_lst\n",
    "\n",
    "dim_red_2 = stop_words_removed_lst\n",
    "\n",
    "dim_red_3 = frequency_filtered_lst\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "pos = list(range(n_groups))\n",
    "bar_width = 0.25\n",
    "\n",
    "opacity = 0.4\n",
    "error_config = {'ecolor': '0.3'}\n",
    "\n",
    "rects1 = plt.bar(pos, \n",
    "                 dim_red_1, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 #color='b',\n",
    "                 label='Initial')\n",
    "\n",
    "rects2 = plt.bar([p + bar_width for p in pos], \n",
    "                 dim_red_2, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 #color='b',\n",
    "                 label='Stop Word Removal')\n",
    "\n",
    "rects3 = plt.bar([p + bar_width*2 for p in pos], \n",
    "                 dim_red_3, bar_width,\n",
    "                 alpha=opacity,\n",
    "                 #color='b',\n",
    "                 label= 'Frequency Filtering')\n",
    "\n",
    "#plt.xlabel('Group')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.title('Counts Over Dimensionality Reduction')\n",
    "plt.xticks(index + bar_width / 2, ('Avg unique per book', \\\n",
    "                        'Avg total per book', 'Total vocab size'))\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import ldamodel\n",
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = ldamodel.LdaModel(corpus=corpus4,alpha='auto', id2word=dictionary2, \\\n",
    "                        num_topics=100, update_every=0, passes=20)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = 'lda.model'\n",
    "lda.save(outputs_dir + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_multi = LdaMulticore(corpus=corpus4,alpha='auto', id2word=dictionary4, \\\n",
    "                        num_topics=100, update_every=0, passes=20, workers=4)\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = 'lda_multi.model'\n",
    "lda_multi.save(outputs_dir + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lda_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the topics and their most significant terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda.show_topics()\n",
    "#also lda.show_topics() - what is difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given document (in bow format), see most relevant topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.get_document_topics(dictionary2.doc2bow(transform_txt_file_v2('1080-clean.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given term in vocab, see what topics are most likely/relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.get_term_topics(100, minimum_probability=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.get_topic_terms(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tup in lda.get_topic_terms(0):\n",
    "    print dictionary2[tup[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Document alignment\n",
    "#softmax - zeroing - zero out all not in the top2 alignment\n",
    "    #manually creat label from top word\n",
    "    #need to include logic to use two topics if below certain threshhold\n",
    "#projection - \n",
    "#cos sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
